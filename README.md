# MarioForge: Player-Rated Procedural Level Generation for a Mario-like Platformer

**Authors:** Antoni Czapski, [Teammate], [Teammate]  
**Course:** Artificial Intelligence <3 Games: Procedural Content Generation (University of Wrocław)  
**Date:** YYYY-MM-DD  
**Code:** (repo link)  
**Demo:** (link, if available)

---

## Abstract
TODO

---

## 1. Introduction
### 1.1 Problem statement
TODO

### 1.2 Why it matters (justification)
TODO

### 1.3 Contributions
- Contribution A: ________
- Contribution B: ________
- Contribution C: ________

---

## 2. Background & related work
### 2.1 PCG for platformer levels
TODO

### 2.2 Evaluation in PCG (automated vs human)
TODO

### 2.3 Prior Mario level generation approaches
TODO

---

## 3. Methodology
### 3.1 Level representation
TODO

### 3.2 Baselines
- Baseline #1: ________
- Baseline #2: ________

### 3.3 Proposed generator
- Core idea: ________
- Constraints: ________
- Objective(s): ________
- Training/search procedure: ________
- Ablations: ________

### 3.4 Player rating platform
- Architecture: frontend/backend/storage
- Generator plugin API
- Matchmaking strategy (which comparisons to show)
- Rating update method (Elo/TrueSkill)
- Anti-abuse / noise handling

---

## 4. Experiments & results
### 4.1 Experimental setup
- Hardware: ________
- Datasets: ________
- Metrics: ________
- Reproducibility: seeds/configs

### 4.2 Automated evaluation results
- Tables/plots: ________

### 4.3 Human evaluation results
- #sessions, #votes, #players: ________
- Ranking stability: ________
- Qualitative analysis: ________

### 4.4 Discussion
- What worked well: ________
- Failure modes: ________
- Limitations: ________

---

## 5. How to run
### 5.1 Install
```bash
# TODO
````

### 5.2 Run generators + evaluation

```bash
# TODO
```

### 5.3 Run the web app locally

```bash
# TODO
```

---

## 6. Conclusions & future work

* Summary of results
* Differences from initial plan
* Next steps if continued

---

## Appendix

* Extra figures
* Example generated levels
* Additional implementation notes

---

## References

TODO (bib)


---

## Next Steps (5)
1) Copy these three files into repo root as-is (`README-INIT.md`, `README-MID.md`, `README.md`) to match the “three markdown reports” expectation.   
2) Replace placeholders: repo link, teammate list, and (optionally) lab assistant name for tracking.  
3) Add a minimal `src/` + `web/` skeleton and put a tiny “hello generator” + “hello web” so the repo is credibly executable early.  
4) Start a `references.bib` (or `docs/refs.md`) and keep it updated as you read papers (saves pain in final week).  
5) Add a `make`/`task` entrypoint plan (“one command runs experiment; one command runs web”) so your final “How to run” section is trivial to complete.
::contentReference[oaicite:16]{index=16}
