{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96686e1",
   "metadata": {},
   "source": [
    "# Grokking on PLGrid (Athena): modular addition with a 1-layer Transformer\n",
    "\n",
    "This notebook trains a small Transformer to learn modular addition, a classic setup where **train accuracy reaches ~100% quickly**, while **test accuracy stays low for a long time** and then suddenly improves (“grokking”).\n",
    "\n",
    "It’s designed to run well on GPU (if available) but also works on CPU. On an HPC cluster, **do the training inside an interactive SLURM allocation** (so you’re not running heavy compute on the login node).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a045d45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 2025-12-23T17:37:21\n",
      "Host: MSI\n",
      "Python: 3.11.9\n",
      "\n",
      "SLURM env:\n",
      "\n",
      "CPU:\n",
      "  logical cores: 12\n",
      "  uname: [unavailable] [WinError 2] Nie można odnaleźć określonego pliku\n",
      "\n",
      "Memory (/proc/meminfo):\n",
      "MemTotal:       16293060 kB\n",
      "MemAvailable:   15814596 kB\n",
      "\n",
      "PyTorch: 2.2.2+cpu\n",
      "CUDA available: False\n",
      "\n",
      "nvidia-smi:\n",
      "GPU 0: NVIDIA GeForce RTX 2060 (UUID: GPU-85ac6bca-37bd-4a95-59ea-4b63be8c07b2)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time, math, json, random, subprocess, platform\n",
    "from datetime import datetime\n",
    "\n",
    "def _run(cmd):\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, text=True)\n",
    "        return out.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[unavailable] {e}\"\n",
    "\n",
    "print(\"Timestamp:\", datetime.now().isoformat(timespec=\"seconds\"))\n",
    "print(\"Host:\", platform.node())\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "\n",
    "# SLURM context (if you're inside an allocation, these will be set)\n",
    "slurm_keys = [\"SLURM_JOB_ID\",\"SLURM_JOB_NAME\",\"SLURM_PARTITION\",\"SLURM_NODELIST\",\"SLURM_CPUS_PER_TASK\",\"SLURM_NTASKS\",\"SLURM_GPUS\",\"CUDA_VISIBLE_DEVICES\"]\n",
    "print(\"\\nSLURM env:\")\n",
    "for k in slurm_keys:\n",
    "    v = os.environ.get(k)\n",
    "    if v is not None:\n",
    "        print(f\"  {k}={v}\")\n",
    "\n",
    "print(\"\\nCPU:\")\n",
    "print(\"  logical cores:\", os.cpu_count())\n",
    "print(\"  uname:\", _run([\"uname\",\"-a\"]))\n",
    "\n",
    "# RAM (best-effort)\n",
    "meminfo = _run([\"bash\",\"-lc\",\"grep -E 'MemTotal|MemAvailable' /proc/meminfo | head -n 2\"])\n",
    "print(\"\\nMemory (/proc/meminfo):\")\n",
    "print(meminfo)\n",
    "\n",
    "# Torch + GPU info\n",
    "import torch\n",
    "print(\"\\nPyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    print(\"\\nnvidia-smi (best-effort):\")\n",
    "    print(_run([\"bash\",\"-lc\",\"nvidia-smi -L && echo '---' && nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader\"]))\n",
    "else:\n",
    "    print(\"\\nnvidia-smi:\")\n",
    "    print(_run([\"bash\",\"-lc\",\"nvidia-smi -L\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e010731",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "If you want a “more compute-heavy” run, increase `max_steps`, `d_model`, and/or `p` (modulus).  \n",
    "Grokking often needs **many steps**, so start with defaults, confirm it runs, then scale up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35308ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CFG(seed=0, p=113, train_frac=0.5, d_model=256, n_heads=4, d_ff=1024, n_layers=1, dropout=0.0, batch_size=1024, lr=0.001, weight_decay=0.01, max_steps=200000, eval_every=200, grad_clip=1.0, train_acc_threshold=0.995, test_acc_threshold=0.99, min_train_streak=5, min_test_streak=5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 0\n",
    "\n",
    "    # Task: modular addition mod p\n",
    "    p: int = 113                 # try 113, 131, 251\n",
    "    train_frac: float = 0.5      # grokking usually appears with partial training set\n",
    "\n",
    "    # Model\n",
    "    d_model: int = 256\n",
    "    n_heads: int = 4\n",
    "    d_ff: int = 1024\n",
    "    n_layers: int = 1            # 1-layer transformer\n",
    "    dropout: float = 0.0\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 1024\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-2\n",
    "    max_steps: int = 200_000\n",
    "    eval_every: int = 200\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # Grokking-aware stopping\n",
    "    train_acc_threshold: float = 0.995\n",
    "    test_acc_threshold: float = 0.99\n",
    "    min_train_streak: int = 5\n",
    "    min_test_streak: int = 5\n",
    "\n",
    "cfg = CFG()\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b895e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 113 vocab: 114 seq_len: 3\n",
      "train: 6384 test: 6385 train_frac: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# Tokens: 0..p-1 are numbers, token p is '='\n",
    "EQ_TOKEN = cfg.p\n",
    "VOCAB = cfg.p + 1\n",
    "SEQ_LEN = 3  # [a, b, '='] -> predict (a+b)%p\n",
    "\n",
    "pairs = [(a,b) for a in range(cfg.p) for b in range(cfg.p)]\n",
    "rng = np.random.default_rng(cfg.seed)\n",
    "rng.shuffle(pairs)\n",
    "\n",
    "n_total = len(pairs)\n",
    "n_train = int(cfg.train_frac * n_total)\n",
    "train_pairs = pairs[:n_train]\n",
    "test_pairs  = pairs[n_train:]\n",
    "\n",
    "def make_xy(pairs):\n",
    "    X = torch.tensor([[a, b, EQ_TOKEN] for a,b in pairs], dtype=torch.long)\n",
    "    y = torch.tensor([(a+b) % cfg.p for a,b in pairs], dtype=torch.long)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = make_xy(train_pairs)\n",
    "X_test, y_test   = make_xy(test_pairs)\n",
    "\n",
    "class ModAdd(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(ModAdd(X_train, y_train), batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader  = DataLoader(ModAdd(X_test, y_test), batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "print(\"p:\", cfg.p, \"vocab:\", VOCAB, \"seq_len:\", SEQ_LEN)\n",
    "print(\"train:\", len(X_train), \"test:\", len(X_test), \"train_frac:\", cfg.train_frac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "675edc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aad7554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Parameters: 0.85M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\OlimpiadaAI\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class OneLayerTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, d_model, n_heads, d_ff, n_layers=1, dropout=0.0, p_out=113):\n",
    "        super().__init__()\n",
    "        self.tok = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = nn.Embedding(seq_len, d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, p_out)\n",
    "\n",
    "        nn.init.normal_(self.tok.weight, std=0.02)\n",
    "        nn.init.normal_(self.pos.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        pos_ids = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        h = self.tok(x) + self.pos(pos_ids)\n",
    "        h = self.enc(h)\n",
    "        h = self.ln(h)\n",
    "        logits = self.head(h[:, -1, :])  # predict using '=' position\n",
    "        return logits\n",
    "\n",
    "model = OneLayerTransformer(\n",
    "    vocab_size=VOCAB,\n",
    "    seq_len=SEQ_LEN,\n",
    "    d_model=cfg.d_model,\n",
    "    n_heads=cfg.n_heads,\n",
    "    d_ff=cfg.d_ff,\n",
    "    n_layers=cfg.n_layers,\n",
    "    dropout=cfg.dropout,\n",
    "    p_out=cfg.p,\n",
    ").to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Parameters:\", f\"{n_params/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d63224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for X, y in loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += (pred == y).sum().item()\n",
    "        total += X.size(0)\n",
    "    return total_loss / total, total_correct / total\n",
    "\n",
    "def train_step(X, y, optimizer):\n",
    "    model.train()\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    logits = model(X)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    if cfg.grad_clip is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "    optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        acc = (pred == y).float().mean().item()\n",
    "    return loss.item(), acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d6af8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\OlimpiadaAI\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step       1] train: loss=4.8888 acc=0.0102 | test: loss=4.9352 acc=0.0074 | watch=False train_streak=0 test_streak=0 | elapsed=0.0 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     train_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_loader)\n\u001b[0;32m     23\u001b[0m     Xb, yb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_iter)\n\u001b[1;32m---> 25\u001b[0m loss, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39meval_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     28\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m evaluate(eval_train_loader)\n",
      "Cell \u001b[1;32mIn[5], line 28\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(X, y, optimizer)\u001b[0m\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y)\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mgrad_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), cfg\u001b[38;5;241m.\u001b[39mgrad_clip)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\OlimpiadaAI\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\OlimpiadaAI\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop with \"grokking-complete\" stopping\n",
    "import pandas as pd\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "log = []\n",
    "t0 = time.time()\n",
    "\n",
    "train_streak = 0\n",
    "test_streak = 0\n",
    "watching_for_grokking = False\n",
    "\n",
    "eval_train_loader = DataLoader(ModAdd(X_train, y_train), batch_size=cfg.batch_size, shuffle=False)\n",
    "eval_test_loader  = test_loader\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "\n",
    "for step in range(1, cfg.max_steps + 1):\n",
    "    try:\n",
    "        Xb, yb = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        Xb, yb = next(train_iter)\n",
    "\n",
    "    loss, acc = train_step(Xb, yb, optimizer)\n",
    "\n",
    "    if step % cfg.eval_every == 0 or step == 1:\n",
    "        tr_loss, tr_acc = evaluate(eval_train_loader)\n",
    "        te_loss, te_acc = evaluate(eval_test_loader)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        log.append({\n",
    "            \"step\": step,\n",
    "            \"train_loss\": tr_loss,\n",
    "            \"train_acc\": tr_acc,\n",
    "            \"test_loss\": te_loss,\n",
    "            \"test_acc\": te_acc,\n",
    "            \"elapsed_s\": elapsed,\n",
    "        })\n",
    "\n",
    "        if tr_acc >= cfg.train_acc_threshold:\n",
    "            train_streak += 1\n",
    "        else:\n",
    "            train_streak = 0\n",
    "\n",
    "        if train_streak >= cfg.min_train_streak:\n",
    "            watching_for_grokking = True\n",
    "\n",
    "        if watching_for_grokking:\n",
    "            if te_acc >= cfg.test_acc_threshold:\n",
    "                test_streak += 1\n",
    "            else:\n",
    "                test_streak = 0\n",
    "\n",
    "        print(f\"[step {step:>7}] \"\n",
    "              f\"train: loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n",
    "              f\"test: loss={te_loss:.4f} acc={te_acc:.4f} | \"\n",
    "              f\"watch={watching_for_grokking} train_streak={train_streak} test_streak={test_streak} | \"\n",
    "              f\"elapsed={elapsed/60:.1f} min\")\n",
    "\n",
    "        if watching_for_grokking and test_streak >= cfg.min_test_streak:\n",
    "            print(\"\\nStopping: grokking appears complete (sustained high test accuracy).\" )\n",
    "            break\n",
    "\n",
    "df = pd.DataFrame(log)\n",
    "df.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e57ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to disk\n",
    "from pathlib import Path\n",
    "out_dir = Path(\"./runs\") / f\"modadd_p{cfg.p}_seed{cfg.seed}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_path = out_dir / \"metrics.csv\"\n",
    "df.to_csv(metrics_path, index=False)\n",
    "\n",
    "cfg_path = out_dir / \"config.json\"\n",
    "cfg_path.write_text(json.dumps(cfg.__dict__, indent=2))\n",
    "\n",
    "print(\"Saved:\", metrics_path)\n",
    "print(\"Saved:\", cfg_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(df) == 0:\n",
    "    raise RuntimeError(\"No metrics logged yet. Increase max_steps or lower eval_every and re-run training.\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df[\"step\"], df[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(df[\"step\"], df[\"test_loss\"], label=\"test_loss\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"cross-entropy loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss vs step\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df[\"step\"], df[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(df[\"step\"], df[\"test_acc\"], label=\"test_acc\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.01)\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy vs step (grokking = delayed test jump)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1823226",
   "metadata": {},
   "source": [
    "## Notes / knobs to turn\n",
    "\n",
    "If you don’t see grokking, it’s usually hyperparameters/time. Try increasing `max_steps`, using `weight_decay` in `[1e-2, 5e-2]`, and trying `p=131` or `p=251`.  \n",
    "For a truly compute-heavy run on a GPU node, `max_steps=500_000` is a reasonable “stretch goal”.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OlimpiadaAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
