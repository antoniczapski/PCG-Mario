{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96686e1",
   "metadata": {},
   "source": [
    "# Grokking on PLGrid (Athena): modular addition with a 1-layer Transformer\n",
    "\n",
    "This notebook trains a small Transformer to learn modular addition, a classic setup where **train accuracy reaches ~100% quickly**, while **test accuracy stays low for a long time** and then suddenly improves (“grokking”).\n",
    "\n",
    "It’s designed to run well on GPU (if available) but also works on CPU. On an HPC cluster, **do the training inside an interactive SLURM allocation** (so you’re not running heavy compute on the login node).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, math, json, random, subprocess, platform\n",
    "from datetime import datetime\n",
    "\n",
    "def _run(cmd):\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, text=True)\n",
    "        return out.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[unavailable] {e}\"\n",
    "\n",
    "print(\"Timestamp:\", datetime.now().isoformat(timespec=\"seconds\"))\n",
    "print(\"Host:\", platform.node())\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "\n",
    "# SLURM context (if you're inside an allocation, these will be set)\n",
    "slurm_keys = [\"SLURM_JOB_ID\",\"SLURM_JOB_NAME\",\"SLURM_PARTITION\",\"SLURM_NODELIST\",\"SLURM_CPUS_PER_TASK\",\"SLURM_NTASKS\",\"SLURM_GPUS\",\"CUDA_VISIBLE_DEVICES\"]\n",
    "print(\"\\nSLURM env:\")\n",
    "for k in slurm_keys:\n",
    "    v = os.environ.get(k)\n",
    "    if v is not None:\n",
    "        print(f\"  {k}={v}\")\n",
    "\n",
    "print(\"\\nCPU:\")\n",
    "print(\"  logical cores:\", os.cpu_count())\n",
    "print(\"  uname:\", _run([\"uname\",\"-a\"]))\n",
    "\n",
    "# RAM (best-effort)\n",
    "meminfo = _run([\"bash\",\"-lc\",\"grep -E 'MemTotal|MemAvailable' /proc/meminfo | head -n 2\"])\n",
    "print(\"\\nMemory (/proc/meminfo):\")\n",
    "print(meminfo)\n",
    "\n",
    "# Torch + GPU info\n",
    "import torch\n",
    "print(\"\\nPyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    print(\"\\nnvidia-smi (best-effort):\")\n",
    "    print(_run([\"bash\",\"-lc\",\"nvidia-smi -L && echo '---' && nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader\"]))\n",
    "else:\n",
    "    print(\"\\nnvidia-smi:\")\n",
    "    print(_run([\"bash\",\"-lc\",\"nvidia-smi -L\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e010731",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "If you want a “more compute-heavy” run, increase `max_steps`, `d_model`, and/or `p` (modulus).  \n",
    "Grokking often needs **many steps**, so start with defaults, confirm it runs, then scale up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35308ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 0\n",
    "\n",
    "    # Task: modular addition mod p\n",
    "    p: int = 113                 # try 113, 131, 251\n",
    "    train_frac: float = 0.5      # grokking usually appears with partial training set\n",
    "\n",
    "    # Model\n",
    "    d_model: int = 256\n",
    "    n_heads: int = 4\n",
    "    d_ff: int = 1024\n",
    "    n_layers: int = 1            # 1-layer transformer\n",
    "    dropout: float = 0.0\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 1024\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-2\n",
    "    max_steps: int = 200_000\n",
    "    eval_every: int = 200\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # Grokking-aware stopping\n",
    "    train_acc_threshold: float = 0.995\n",
    "    test_acc_threshold: float = 0.99\n",
    "    min_train_streak: int = 5\n",
    "    min_test_streak: int = 5\n",
    "\n",
    "cfg = CFG()\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b895e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# Tokens: 0..p-1 are numbers, token p is '='\n",
    "EQ_TOKEN = cfg.p\n",
    "VOCAB = cfg.p + 1\n",
    "SEQ_LEN = 3  # [a, b, '='] -> predict (a+b)%p\n",
    "\n",
    "pairs = [(a,b) for a in range(cfg.p) for b in range(cfg.p)]\n",
    "rng = np.random.default_rng(cfg.seed)\n",
    "rng.shuffle(pairs)\n",
    "\n",
    "n_total = len(pairs)\n",
    "n_train = int(cfg.train_frac * n_total)\n",
    "train_pairs = pairs[:n_train]\n",
    "test_pairs  = pairs[n_train:]\n",
    "\n",
    "def make_xy(pairs):\n",
    "    X = torch.tensor([[a, b, EQ_TOKEN] for a,b in pairs], dtype=torch.long)\n",
    "    y = torch.tensor([(a+b) % cfg.p for a,b in pairs], dtype=torch.long)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = make_xy(train_pairs)\n",
    "X_test, y_test   = make_xy(test_pairs)\n",
    "\n",
    "class ModAdd(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(ModAdd(X_train, y_train), batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader  = DataLoader(ModAdd(X_test, y_test), batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "print(\"p:\", cfg.p, \"vocab:\", VOCAB, \"seq_len:\", SEQ_LEN)\n",
    "print(\"train:\", len(X_train), \"test:\", len(X_test), \"train_frac:\", cfg.train_frac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class OneLayerTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, d_model, n_heads, d_ff, n_layers=1, dropout=0.0, p_out=113):\n",
    "        super().__init__()\n",
    "        self.tok = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = nn.Embedding(seq_len, d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, p_out)\n",
    "\n",
    "        nn.init.normal_(self.tok.weight, std=0.02)\n",
    "        nn.init.normal_(self.pos.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        pos_ids = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        h = self.tok(x) + self.pos(pos_ids)\n",
    "        h = self.enc(h)\n",
    "        h = self.ln(h)\n",
    "        logits = self.head(h[:, -1, :])  # predict using '=' position\n",
    "        return logits\n",
    "\n",
    "model = OneLayerTransformer(\n",
    "    vocab_size=VOCAB,\n",
    "    seq_len=SEQ_LEN,\n",
    "    d_model=cfg.d_model,\n",
    "    n_heads=cfg.n_heads,\n",
    "    d_ff=cfg.d_ff,\n",
    "    n_layers=cfg.n_layers,\n",
    "    dropout=cfg.dropout,\n",
    "    p_out=cfg.p,\n",
    ").to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Parameters:\", f\"{n_params/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d63224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for X, y in loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += (pred == y).sum().item()\n",
    "        total += X.size(0)\n",
    "    return total_loss / total, total_correct / total\n",
    "\n",
    "def train_step(X, y, optimizer):\n",
    "    model.train()\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    logits = model(X)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    if cfg.grad_clip is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "    optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        acc = (pred == y).float().mean().item()\n",
    "    return loss.item(), acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6af8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with \"grokking-complete\" stopping\n",
    "import pandas as pd\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "log = []\n",
    "t0 = time.time()\n",
    "\n",
    "train_streak = 0\n",
    "test_streak = 0\n",
    "watching_for_grokking = False\n",
    "\n",
    "eval_train_loader = DataLoader(ModAdd(X_train, y_train), batch_size=cfg.batch_size, shuffle=False)\n",
    "eval_test_loader  = test_loader\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "\n",
    "for step in range(1, cfg.max_steps + 1):\n",
    "    try:\n",
    "        Xb, yb = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        Xb, yb = next(train_iter)\n",
    "\n",
    "    loss, acc = train_step(Xb, yb, optimizer)\n",
    "\n",
    "    if step % cfg.eval_every == 0 or step == 1:\n",
    "        tr_loss, tr_acc = evaluate(eval_train_loader)\n",
    "        te_loss, te_acc = evaluate(eval_test_loader)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        log.append({\n",
    "            \"step\": step,\n",
    "            \"train_loss\": tr_loss,\n",
    "            \"train_acc\": tr_acc,\n",
    "            \"test_loss\": te_loss,\n",
    "            \"test_acc\": te_acc,\n",
    "            \"elapsed_s\": elapsed,\n",
    "        })\n",
    "\n",
    "        if tr_acc >= cfg.train_acc_threshold:\n",
    "            train_streak += 1\n",
    "        else:\n",
    "            train_streak = 0\n",
    "\n",
    "        if train_streak >= cfg.min_train_streak:\n",
    "            watching_for_grokking = True\n",
    "\n",
    "        if watching_for_grokking:\n",
    "            if te_acc >= cfg.test_acc_threshold:\n",
    "                test_streak += 1\n",
    "            else:\n",
    "                test_streak = 0\n",
    "\n",
    "        print(f\"[step {step:>7}] \"\n",
    "              f\"train: loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n",
    "              f\"test: loss={te_loss:.4f} acc={te_acc:.4f} | \"\n",
    "              f\"watch={watching_for_grokking} train_streak={train_streak} test_streak={test_streak} | \"\n",
    "              f\"elapsed={elapsed/60:.1f} min\")\n",
    "\n",
    "        if watching_for_grokking and test_streak >= cfg.min_test_streak:\n",
    "            print(\"\\nStopping: grokking appears complete (sustained high test accuracy).\" )\n",
    "            break\n",
    "\n",
    "df = pd.DataFrame(log)\n",
    "df.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e57ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to disk\n",
    "from pathlib import Path\n",
    "out_dir = Path(\"./runs\") / f\"modadd_p{cfg.p}_seed{cfg.seed}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_path = out_dir / \"metrics.csv\"\n",
    "df.to_csv(metrics_path, index=False)\n",
    "\n",
    "cfg_path = out_dir / \"config.json\"\n",
    "cfg_path.write_text(json.dumps(cfg.__dict__, indent=2))\n",
    "\n",
    "print(\"Saved:\", metrics_path)\n",
    "print(\"Saved:\", cfg_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(df) == 0:\n",
    "    raise RuntimeError(\"No metrics logged yet. Increase max_steps or lower eval_every and re-run training.\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df[\"step\"], df[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(df[\"step\"], df[\"test_loss\"], label=\"test_loss\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"cross-entropy loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss vs step\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df[\"step\"], df[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(df[\"step\"], df[\"test_acc\"], label=\"test_acc\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.01)\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy vs step (grokking = delayed test jump)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1823226",
   "metadata": {},
   "source": [
    "## Notes / knobs to turn\n",
    "\n",
    "If you don’t see grokking, it’s usually hyperparameters/time. Try increasing `max_steps`, using `weight_decay` in `[1e-2, 5e-2]`, and trying `p=131` or `p=251`.  \n",
    "For a truly compute-heavy run on a GPU node, `max_steps=500_000` is a reasonable “stretch goal”.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
